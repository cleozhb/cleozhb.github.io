---
title: CUDA性能指标
date: 2018-05-17 10:13:38
tags: CUDA
---

共享内存被分配在SM上的常驻线程块中，寄存器在线程中被分配。寄存器和共享内存是SM中的稀缺资源。CUDA将这些资源分配到SM中的所有常驻线程里。

<!--more-->

| 参数          | 指令                |
| ------------- | ------------------- |
| 观察线程束分化 | nvprof --metrics branch_efficiency | 
| 获得分支和分化分支的事件计数器 | nvprof --events branch,divergent_branch | 
| 检测活跃的线程束(一个内核的可实现占用率) | nvprof --metrics achieved_occupacy | 
| 检测内核的内存读取效率 | nvprof --metrics gld_throughput | 
| 检测全局加载效率 | nvprof --metrics gld_efficiency  | 
| 查看每个线程束上执行指令数量的平均值 | nvprof --metrics inst_per_warp | 
| 查看因为同步占用的时间 | nvprof --metrics stall_sync | 
| 检测内存加载/存储效率指标 | nvprof --metrics gld_efficiency,gst_efficiency | 
| 设备内存读取吞吐量指标 | nvprof --metrics dram_read_throughput | 


所需线程束的数量=延迟×吞吐量
对内存操作来说，所需要的并行可以表示为在每个周期隐藏内存延迟所需要的字节数。内存带宽是针对整个设备而言的。也就是说假设一个Fermi架构的GPU，
指令延迟（周期）800
吞吐量（GB/s）144
内存频率（GHz）1.566
带宽（B/周期）92
那么要充分利用其内存吞吐，需要多少内存操作以支持并行呢？
144GB/s   /    1.566GHz   =   92 字节/周期
92×800/1024 = 74KB
所以Fermi内存操作所需要的并行接近74KB的内存I/O运行，才能充分利用。假设每个线程都将一个4字节的数据从全局内存移动到SM中用于计算，则总共需要74×1024/4 = 18944个线程;18944  /  32  =  592个线程束来隐藏所有的内存延迟;
Fermi架构有16个SM，因此需要592/16=37个线程束来隐藏所有的内存延迟。


后两种情况中最内层维数中块的大小是线程束的一半，这样就只利用了半个线程束大小的线程。


| 执行时间(ms) | 线程块维数 | 可实现占用率 | 内存操作效率(GB/s) | 全局加载效率 | 
| ---------- | ---------- | ---------- | ---------- | ---------- | 
| 60 | （32，32 ） | 0.501 | 35.9 | 1 | 
| 38 | （32，16 ） | 0.7369 | 56.478 | 1 | 
| 51 | （16，32 ） | 0.766 | 85.195 | 0.4996 | 
| 46 | （16，16 ） | 0.8106 | 94.708 | 0.498 | 
| 33 | （64,2） | 0.55 |    |    | 
| 34 | （64,4） | 0.798 |    |    | 
| 36 | （64,8） | 0.753 |    |    | 
| 32.68 | （128,2） | 0.802 |    |    | 
| 34 | （128,4） | 0.746 |    |    | 
| 46 | （128,8） | 0.573 |    |    | 
| 32.79 | （256,2） | 0.76 |    |    | 
| 38 | （256.4） | 0.595 |    |    | 
| 32.6 | （128,1） |    |    |    | 
| 30.96 | （256,1） | 0.8086 | 69.76 | 1 | 



1、用nvprof检测活跃的线程束
内核的**可实现占用率**是每个周期内活跃线程束的平均数量与一个SM支持的线程束最大数量的比值。nvprof --metrics achieved_occupacy
假设现在有4中线程块的配置方式如下，最后一列为可实现占用率。第二种情况中的块数比第一种情况多，设备有更多活跃的线程束。第四种情况有最高的可实现占用率，但不是最快的，**更高的占用率并不代表的更高的性能**，有其他的因素限制GPU的性能。

2、用nvprof检测内存操作
* 用nvprof --metrics gld_throughput检测内核的内存读取效率，得到每个执行配置的差异。其中第四种情况的加载吞吐量最高。第二种情况的加载吞吐量大约是第四种情况的一般，但却更快。所以**更高的加载吞吐量并不一定意味着更高的性能**。
* 用nvprof --metrics gld_efficiency 检测全局加载效率，即被请求的全局加载吞吐量占所需全局加载吞吐量的壁纸。它衡量了应用程序的加载操作利用设备内存带宽的程度。后两种情况的加载效率是前面两种的一半。这可以解释为什么后两种情况下更高的加载吞吐量和可实现占用率没有产生更好的性能。因为尽管在最后两种情况下正在执行的加载数量（即吞吐量）很多，但是加载的有效性（效率）很低。
* 设备内存读取吞吐量指标，用nvprof --metrics dram_read_throughput比如在循环展开操作中就可以用这个指标来观察展开后的读取吞吐量的指标。

3、增大并行性
（128,2）和（256,2）有最高的性能配置，几乎有相同的可实现占用率。下面通过将这两种情况的block.y设置为1来增大块间并行性，观察性能如何变化。结果发现这样做使得每个线程块的大小减少，从而有更多的线程块被启动来处理相同数量的数据，得到了更好的结果。值得注意的是最好的执行配置不具有最高的可实现占用率，也不具有最高的加载吞吐量。所以没有一个单独的指标能直接优化性能。需要在几个相关的指标之间寻找一个前当的平衡来达到最佳的总体性能。


**指标与性能**
* **在大多数情况下一个单独的指标不能产生最佳性能**
* **与总体性能最相关的指标或事件取决于内核代码的本质**
* **在相关的指标与事件之间寻求一个好的平衡**

指标
* 分支效率：未分化的分支与全部分支之比
* 吞吐量：已经达到的值。用来描述单位时间内任何形式的信息或操作的执行速度。例如：每个周期完成了多少个指令。
* 带宽：理论峰值。通常用来描述单位时间内最大可能的数据传输量。
* 占用率：每个SM中活跃的线程束占最大线程束数量的比值。


GPU与CPU之间的通信以及同步。一个流中的函数是串行执行的
GPU-GPU
GPU-CPU
CPU-GPU

当执行异步数据传输时，必须使用固定（非分页）主机内存。
在主机虚拟内存中固定分配，可以确保其在CPU内存中的物理位置在应用程序的整个生命周期中保持不变。否则，操作系统可以随时自由改变主机虚拟内存的物理位置。如果没有固定主机内存的情况下执行一个异步的CUDA转移操作，操作系统可能会在物理层面移动数组，而CUDA操作运行时将该数组移动到设备中，这样会导致未定义的行为。
