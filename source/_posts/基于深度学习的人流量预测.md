---
title: 基于深度学习的人流量预测
date: 2017-09-12 08:13:26
categories: 论文笔记 
tags: [DNN,CNN]
---
该研究论文《Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction》已经发表在 AAAI 2017。是深度学习用在城市计算领域中首次在高等级会议上发表。<br>
### 深度时空残差网络ST-ResNet(Deep Spatio-temporal residual network)
![时空模型结构](http://oqadn1oza.bkt.clouddn.com/%E6%80%BB%E4%BD%93%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png)
<!--more-->
&emsp;&emsp; 论文使用了北京市的出租车 GPS 轨迹上面做了验证，大概用了约 3 万多辆出租车长达 5 年的数据，同时也利用纽约公开的自行车租赁数据做了验证。将这些琐碎的数据转变成有规律的、能够对深度学习作为输入的格式，通过空间的划分和折射，变成一个二维矩阵，使它能够作为深度学习模型的输入。这个转换过程很重要，涉及到不同数据之间的多元化融合。具体而言，论文中将城市划分成均匀且不相交的网格，比如划分成一个个一平方公里的网格，然后输入人流数据（包括手机、出租车轨迹等）投射在网格里面，计算出每个格子里有多少人进和出。红色越亮的地方就表示人越多，一帧的图像比如说是二维图像，如果有很多时间点就可以持续生成图片，同时有对应的事件和天气信息，这就构成了数据的输入，把时空数据转换成这样一个模式。
## 模型总架构：
&emsp;&emsp;模型包括4个主要部分来对时空数据的周期性、趋势性、平滑性和外部数据的影响分别建模。上图中可以看到，首先将每个格网中的流入人流量和流出人流量分别转化为一个有两个信道的图像矩阵。将时间轴提取3类信息，包括了近期、中长期、长期。将各个时间戳下包含输入输出2个信道的这三类数据分别喂给三个相同参数的convolutionalNN。紧接着是深度残差网络。这样的结构可以学习到从近处到远处大范围的时空依赖关系。外部数据包括一些人工提取的天气、事件等数据，将这些数据喂给fully-connection neural network。上面那三个输出先进行一次融合，融合时考虑到每个区域事件范围作用强度不同，对于不同区域给三种数据不同的权重。然后将融合结果与外部数据的结果进行进一步融合。
![CNN & residual模型结构](http://oqadn1oza.bkt.clouddn.com/%E5%8D%B7%E7%A7%AF%E7%BB%93%E6%9E%84%E5%92%8C%E6%AE%8B%E5%B7%AE%E7%BB%93%E6%9E%84%E5%9B%BE.png)
## 卷积层架构：
&emsp;&emsp;一个城市通常很大，在图上由很多pixel组成，直觉上，人流量会受到周免邻近区域的影响，所以CNN可以很好的抓取分层的空间结构特征。现在地铁的发展已将将这种影响扩展到更远的区域了。我们要设计一个深层的CNN，因为单层CNN受卷积核大小的限制，只考虑到相邻的单元。卷积层的输入数据中，每一层可以看做是一个2*time_interval×I×J的图像。有2×time_interval这么多个信道，大小为I*J个pixel。
![卷积公式](http://oqadn1oza.bkt.clouddn.com/%E5%8D%B7%E7%A7%AF%E5%85%AC%E5%BC%8F.png)

## 残差网络层：
&emsp;&emsp;在卷积神经网络中我们往往需要深层的CNN来学习更大范围。例如要用3×3的kenel来学习32×32的图像，需要15层卷积层堆叠。为了解决因为深度增加而导致性能下降问题，使用深度残差网络，可以用来训练很深的网络，网络深度甚至可以大于1000。具体介绍见文末。
&emsp;&emsp;
## 创新点：
1. ST-ResNet使用了卷积残差网络来进行空间属性模拟。深度卷积神经网络的过程就是把区域划成格子之后，对相关区域进行卷积运算得到一个值，可以认为，通过一次卷积之后把周围区域人流的相关性抓住了，卷积多次后把更远地方的区域属性都卷积到一起，如果你想捕捉很远的地方，意味着你的卷积网络层次必须要比较深，只有一层抓不到很远地方的相关性。之所以要这样做，是因为之前有提到，很多人可能从外地很远的地方通过高铁或高速公路直接抵达，不会经过你所覆盖到的周边区。<br>
此外，一旦网络层次比较深，训练会变得非常复杂，基于卷积神经网络，文中引入了深度残差网络结构来做人流量预测，用来帮助深度卷积网络提高训练精度。<br>

2. 提取了三个维度下人流量数据的特征，平滑性、周期性和趋势性。分别利用三个深度残差网络来对这些属性进行建模。基于对时空数据深刻的理解，论文中提出一个特殊的网络模型。在这个结构里，只需要抽取一些关键帧，比如说昨天同一时刻，前天同一时刻，其他时间不做输入，大概只要用几十帧的关键帧作为输入，就可以体现出几个月里所包含的周期性和趋势性，使得的网络结构大大简化。有了这样的数据之后，再对时间特性进行模拟。把最近几个小时、几帧的数据，输入到时空残差网络里面，模拟相邻时刻变化的平滑过程，然后把对应时间点昨天、前天的数据输入来模拟周期性，再把更远的时间点对应的读数拿进来，模拟一个趋势性，分别模拟了三个时间属性。

3. 在融合相似性、周期性和趋势性这三个模块时，相比于直接的融合，提出的基于参数矩阵的融合方法考虑到了每个区域的时间特性的强度不同给予不同的权重，因此取得了更好的结果。<br>
![三种时间模型结果图](http://oqadn1oza.bkt.clouddn.com/%E4%B8%89%E7%A7%8D%E6%97%B6%E9%97%B4%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%9C%E5%9B%BE.png)
![模型融合公式](http://oqadn1oza.bkt.clouddn.com/3%E4%B8%AA%E6%97%B6%E9%97%B4%E8%9E%8D%E5%90%88.png)
4. 再把外部事件、天气等因素拿进来进行二次融合，得到一个结果。多元数据融合依旧是个值得探索的领域。

## 深度残差网络：
&emsp;&emsp; 实践中发现网络的深度和模型的效果并不是正相关的关系，导致这个问题的原因并不是由过拟合造成的，很多实践表明在训练深度神经网络的时候，训练误差和测试误差都随着网络深度的增加而增加。导致这个问题可能的原因：<br>
* 很有可能是梯度消失/爆炸引起的
* 新加的层表达能力有限，对前面几层学好的模型是一种损害，所以可能存在一个「最优层数」。
![residual模型图](http://oqadn1oza.bkt.clouddn.com/%E6%AE%8B%E5%B7%AE%E5%8D%95%E5%85%83%E7%A4%BA%E6%84%8F%E5%9B%BE.png)
假定某段神经网络的输入是 x，期望输出是 H(x)，即 H(x) 是期望的复杂潜在映射，但学习难度大；如果我们直接把输入x传到输出作为初始结果，通过下图“shortcut connections”，那么此时我们需要学习的目标就是 F(x)=H(x)-x，于是 ResNet 相当于将学习目标改变了，不再是学习一个完整的输出，而是最优解 H(X) 和全等映射 x 的差值，即残差F(X)=H(X)-X。Shortcut 原意指捷径，在这里就表示越层连接，在Highway Network 在设置了一条从 x 直接到 y 的通路，以 T(x, Wt) 作为 gate 来把握两者之间的权重；而 ResNet shortcut 没有权值，传递 x 后每个模块只学习残差F(x)，且网络稳定易于学习，作者同时证明了随着网络深度的增加，性能将逐渐变好。可以推测，当网络层数够深时，优化 Residual Function：F(x)=H(x)−x，易于优化一个复杂的非线性映射 H(x)。<br>
下图所示为 VGGNet-19，以及一个34层深的普通卷积网络，和34层深的 ResNet 网络的对比图。可以看到普通直连的卷积神经网络和 ResNet 的最大区别在于，ResNet 有很多旁路的支线将输入直接连到后面的层，使得后面的层可以直接学习残差，这种结构也被称为 shortcut connections。传统的卷积层或全连接层在信息传递时，或多或少会存在信息丢失、损耗等问题。ResNet 在某种程度上解决了这个问题，通过直接将输入信息绕道传到输出，保护信息的完整性，整个网络则只需要学习输入、输出差别的那一部分，简化学习目标和难度。
![残差网络对比图](http://oqadn1oza.bkt.clouddn.com/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%E5%AF%B9%E6%AF%94%E5%9B%BE.png)
&emsp;&emsp;它主要思想很简单，打破了传统的神经网络n-1层的输出只能给n层作为输入的惯例，使某一层的输出可以直接跨过几层作为后面某一层的输入。
![deep层数对结果的影响](http://oqadn1oza.bkt.clouddn.com/deep%E5%B1%82%E6%95%B0%E5%BD%B1%E5%93%8D.png)
&emsp;&emsp;上图就是其构造深度残差网络的构思来源图，一个是56层的网络一个是20层的网络，从原理上来说其实56层网络的解空间是包括了20层网络的解空间的，换而言之也就是说，56层网络取得的性能应该大于等于20层网络的性能的。但是从训练的迭代过程来看，56层的网络无论从训练误差来看还是测试误差来看，误差都大于20层的网络（这也说明了为什么这不是过拟合现象，因为56层网络本身的训练误差都没有降下去）。导致这个原因就是虽然56层网络的解空间包含了20层网络的解空间，但是我们在训练网络用的是随机梯度下降策略，往往解到的不是全局最优解，而是局部的最优解，显而易见56层网络的解空间更加的复杂，所以导致使用随机梯度下降算法无法解到最优解。
&emsp;&emsp;其实深度残差网络的网络结构能够让一部分的数据可以跳过某些变换层，而直接到后面的层中去。从大量的实验中，我感觉这两种网络只有在很深的场景中才能发挥出“威力”，如果本身网络层数较浅，勉强使用这两种结构是很难得到好的结果的。

参考文献：
1. [深度残差网络和Highway网络](http://m.blog.csdn.net/xiaocong1990/article/details/72499047)
2. [专访 | 微软亚洲研究院郑宇：用人工智能进行城市人流预测](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650723081&idx=2&sn=19a3ea9f1e4fbce9133932d74f6d5429&chksm=871b1777b06c9e61f9701c389d198a064ffdb4175ee4c66120c504ffbe014275dfbb2b82b0f3&mpshare=1&scene=1&srcid=0210Cpx38s7lP5HZ3Oaw7Omk&pass_ticket=yQiyvLfZam1x%2BD7r4Of02uamLCMD3SMvqi0SP4aC6p696nKNGeXwYHVEo54u9R1X#rd)
3. [人流量千变万化难以预测，但微软通过大数据和人工智能做到了](http://www.thepaper.cn/newsDetail_forward_1617662)
4. [MarkDown添加Latex公式](http://www.cnblogs.com/peaceWang/p/Markdown-tian-jia-Latex-shu-xue-gong-shi.html)
5. [ CNN卷积神经网络_深度残差网络 ResNet](http://blog.csdn.net/diamonjoy_zone/article/details/70904212)